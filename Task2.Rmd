---
title: "Task 2 - Tokenization"
author: "Pedro Luis Recio"
date: "18 de octubre de 2017"
output: html_document
---

```{r echo=FALSE, include=FALSE}
library(tm)
library(knitr)
library(ggplot2)
#dir<- "E:/COURSERA/Data Science/Course10-Capstone/CORP/"
#setwd(dir)
opts_knit$set(root.dir = "E:/COURSERA/Data Science/Course10-Capstone/CORP")
tsum <- data.frame()
```

```{r}
# Initial setup variable
dir<- "E:/COURSERA/Data Science/Course10-Capstone/CORP"
setwd(dir)
```




```{r}
# Corpus
fullCorpus <- Corpus(DirSource("E:/COURSERA/Data Science/Course10-Capstone/SAMPLE"), readerControl = list(language="en_US"))

```


```{r}
tk <- function(fcorpus)
{
library(tm)
fcorpus <- tm_map(fcorpus, removeNumbers)
fcorpus <- tm_map(fcorpus, removePunctuation)
fcorpus <- tm_map(fcorpus, tolower)
return(fcorpus)

}  
```



```{r}
# create & apply function to remove special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

docs <- tm_map(fullCorpus, toSpace, "/|@|\\|")

# remove numbers as they are likely not going to provide added benefit to my analysis
docs <- tm_map(docs, removeNumbers)

```

```{r}
docs <- tk(fullCorpus)
```

