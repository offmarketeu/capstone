---
title: "Text prediction algorithm"
author: "Pedro Luis Recio"
date: "13 de octubre de 2017"
output: html_document
---

```{r echo=FALSE, include=FALSE}
# library packages
library(tm)

```

```{r}
# Initial setup variable
dir<- "E:/COURSERA/Data Science/Course10-Capstone/CORP"
setwd(dir)
```


## Intro
 
This is the first task in the Capstone 10 - Data Science project. There are three txt files which have english text from three different sources: Blog, News and Twitter.
Our first task is cleaning and preparing the information for further analysis.
The size of the whole files are huge, so we are going to extract some samples for our analysis. We are generating some binary ramdom  numbers to extract the 10% of the information.

```{r echo=TRUE, warning=FALSE}

# Blogs
blogs <- readLines("E:/COURSERA/Data Science/Course10-Capstone/CORP/en_US.blogs.txt")
set.seed(123)
blogs <- blogs[rbinom(length(blogs)*.10, length(blogs), .5)]
write.csv(blogs, file = "E:/COURSERA/Data Science/Course10-Capstone/sample/blog.sample.csv", row.names = FALSE, col.names = FALSE)

# News
news <- readLines("E:/COURSERA/Data Science/Course10-Capstone/CORP/en_US.news.txt")
set.seed(123)
news <- news[rbinom(length(news)*.10, length(news), .5)]
write.csv(news, file = "E:/COURSERA/Data Science/Course10-Capstone/Sample/news.sample.csv", row.names = FALSE, col.names = FALSE)

# Twitter
twitter <- readLines("E:/COURSERA/Data Science/Course10-Capstone/CORP/en_US.twitter.txt")
set.seed(123)
twitter <- twitter[rbinom(length(twitter)*.10, length(twitter), .5)]
write.csv(twitter, file = "E:/COURSERA/Data Science/Course10-Capstone/Sample/twitter.sample.csv", row.names = FALSE, col.names = FALSE)

# clean up global environment
rm(blogs, news, twitter)

# Corpus
fullCorpus <- Corpus(DirSource("E:/COURSERA/Data Science/Course10-Capstone/"), readerControl = list(language="en_US"))


```

## Exploratory analysis

```{r echo=TRUE, warning=FALSE}

```



## Tokenization

Preparing the information for the analysys is an essential task. For that purpose we need removing numbers, stopwords, and numbers. We would like to remove profanity world for our text too.

Tasks to accomplish

- Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
- Profanity filtering - removing profanity and other words you do not want to predict.


```{r}
# create & apply function to remove special characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

docs <- tm_map(full, toSpace, "/|@|\\|")

# remove numbers as they are likely not going to provide added benefit to my analysis
docs <- tm_map(docs, removeNumbers)

# for my initial analysis I will remove punctuation
docs <- tm_map(docs, removePunctuation)

# transform words to lower case
docs <- tm_map(docs, tolower)

# remove profanity words
profanity <- read.csv("~/Desktop/Personal/Education & Training/Coursera/Capstone/Capstone/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/en", header=FALSE, stringsAsFactors=FALSE)
profanity <- profanity$V1
docs <- tm_map(docs, removeWords, profanity)

# remove white space
docs <- tm_map(docs , stripWhitespace)

# for my initial analysis I will remove stopwords; this eliminates overly common words that add little value in my understanding of the document(s) context.  Later on I will keep stopwords in when performing tokenization and analyzing for n-grams.
docs_context <- tm_map(docs, removeWords, stopwords("english"))

# perform stemming to remove complexity introduced by multiple suffixes and get word radicals
docs_context <- tm_map(docs_context, stemDocument, language = "english")

# reformat documents to PlainTextDocument to allow for DocumentTermMatrix and TermDocumentMatrix formats
docs_context <- tm_map(docs_context, PlainTextDocument)

```

