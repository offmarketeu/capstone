---
title: "Capstone"
author: "PRO"
date: "13 de octubre de 2017"
output: html_document
---

```{r, echo=FALSE, include=FALSE}
library(tm)
library(knitr)
library(ggplot2)
library(NLP)
library(quanteda)
#dir<-"E:/COURSERA/Data Science/Course10-Capstone/"
#dir1<-"E:/COURSERA/Data Science/Course10-Capstone/CORP"
#dir2<-"E:/COURSERA/Data Science/Course10-Capstone/SAMPLE"

dir<-"C:/Users/n040485/Documents/000_CAPSTONE/"
dir1<-"C:/Users/n040485/Documents/000_CAPSTONE/CORP"
dir2<-"C:/Users/n040485/Documents/000_CAPSTONE/SAMPLE"

opts_knit$set(root.dir = dir1)
tsum <- data.frame()

```

## Intro
 
The first step in analyzing any new data set is figuring out.
This exercise uses the files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora. See the About the Corpora reading for more details. The files have been language filtered but may still contain some foreign text.

The Original link of [data base](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

We will use the English database but may consider three other databases in German, Russian and Finnish.




```{r, echo=FALSE,include=FALSE, warning=FALSE}

# Corpus
fullCorpus <- Corpus(DirSource(dir2), readerControl = list(language="en_US"))
fullcorpues<-corpus(fullCorpus)

```


## Tokenization

Preparing the information for the analysys is an essential task. For that purpose we need removing numbers, stopwords, and numbers. We would like to remove profanity world for our text too.

Tasks to accomplish

- Tokenization : identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
- Profanity filtering : removing profanity and other words you do not want to predict.


```{r, echo=FALSE, warning=FALSE}
# Profanity

#Profanity file
profanity <- read.csv(paste(dir,"profanity_list.csv", sep=""), header=FALSE, stringsAsFactors=FALSE)
profanity <- profanity$V1
```


```{r}
# Tokenizations 1-gram

#1-grams with stopwords
tkdfm <- dfm(fullcorpues, tolower=TRUE, remove_numbers = TRUE, remove_punct=TRUE,stem = TRUE, ngrams = 1, verbose = FALSE)
top_ng1<- topfeatures(tkdfm)
 #freq_1<-docfreq(top_ng1)
freq_1 <- data.frame(word=names(top_ng1), count=top_ng1)

#Unigrams graphics
#a
plot1<-ggplot(freq_1,aes(x=reorder(word, -count), y=count,fill=as.factor(word)))+ geom_bar(stat="identity")+ xlab('Count') + ylab('Words (unigrams)') + ggtitle('Top 10 Unigrams Including Stop Words')


#1-grams without stopwords
tkdfm_nst <- dfm(fullcorpues, tolower=TRUE, remove=stopwords("english"), remove_numbers = TRUE, remove_punct=TRUE,stem = TRUE, ngrams = 1, verbose = FALSE)
top_ng_nst<- topfeatures(tkdfm_nst)
freq_1nst <- data.frame(word=names(top_ng_nst), count=top_ng_nst)

#b
ggplot(freq_1nst,aes(x=reorder(word, -count), y=count,fill=as.factor(word)))+ geom_bar(stat="identity")+ xlab('Count') + ylab('Words (unigrams)') + ggtitle('Top 10 Unigrams non Including Stop Words') 

```


```{r}
#2-grams
tkdfm2 <- dfm(fullcorpues, tolower=TRUE, remove_numbers = TRUE, remove_punct=TRUE,stem = TRUE, ngrams = 2, verbose = FALSE)
top_ng2<- topfeatures(tkdfm2)
freq_2 <- data.frame(word=names(top_ng2), count=top_ng2)

#Bigrams graphics
ggplot(freq_2,aes(x=reorder(word, -count), y=count,fill=as.factor(word)))+ geom_bar(stat="identity")+ xlab('Count') + ylab('Words (2-grams)') + ggtitle('Top 10 2-grams Including Stop Words') 

```

```{r}
#3-grams

tkdfm3 <- dfm(fullcorpues, tolower=TRUE, remove_numbers = TRUE, remove_punct=TRUE,stem = TRUE, ngrams = 3, verbose = FALSE)
top_ng3<- topfeatures(tkdfm3)

freq_3 <- data.frame(word=names(top_ng3), count=top_ng3)

#Trigrams graphics
ggplot(freq_3,aes(x=reorder(word, -count), y=count,fill=as.factor(word)))+ geom_bar(stat="identity")+ xlab('Count') + ylab('Words (3-grams)') + ggtitle('Top 10 3-grams Including Stop Words') 



```

# Next steps

